{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "81532e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ RNN -------------------------------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18a42cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf99e6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa89f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as DataLoader\n",
    "import torchvision.datasets as datase\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f19be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fully Connected Network\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d774a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "510dee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparamteres\n",
    "input_size  = 784\n",
    "num_classes = 10\n",
    "learning_rate = .001\n",
    "batch_size = 64\n",
    "epoch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "2b3f2ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_set = datasets.MNIST(root = 'dataset/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "test_set = datasets.MNIST(root = 'dataset/', train = False, transform = transforms.ToTensor(), download = True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aad6df20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Network\n",
    "model = NN(input_size = input_size, num_classes = num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc129110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e47a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_ix , (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device = device)\n",
    "        targets = targets.to(device = device)\n",
    "        \n",
    "        # Get to correct shape\n",
    "        data = data.reshape(data.shape[0], -1)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2c64bb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got57108/ 60000 with accuracy  95.18\n",
      "Got9469/ 10000 with accuracy  94.69\n"
     ]
    }
   ],
   "source": [
    "# Check accuracy on training and test how good our model is\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device = device)\n",
    "            y = y.to(device = device)\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f'Got{num_correct}/ {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100 : .2f}')\n",
    "\n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "12a1d598",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 28\n",
    "sequence_length = 28\n",
    "num_layers = 2\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "learning_rate = .0001\n",
    "batch_size = 64\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a746b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size * sequence_length, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        out    = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "269cfba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size * sequence_length, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.gru(x, h0)\n",
    "        out    = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dd62b628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True)\n",
    "        self.fc  = nn.Linear(hidden_size, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        # out    = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c89f9e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.blstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first = True, bidirectional = True)\n",
    "        self.fc  = nn.Linear(hidden_size * 2, num_classes) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        out, _ = self.blstm(x, (h0, c0))\n",
    "        # out    = out.reshape(out.shape[0], -1)\n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6e20fc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM(input_size, hidden_size, num_layers, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ff2c9454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got57114/ 60000 with accuracy  95.19\n",
      "Got9525/ 10000 with accuracy  95.25\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_ix , (data, targets) in enumerate(train_loader):\n",
    "        \n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device = device).squeeze(1)\n",
    "        targets = targets.to(device = device)\n",
    "        \n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        # Backward    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient Descent\n",
    "        optimizer.step()\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device = device).squeeze(1)\n",
    "            y = y.to(device = device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f'Got{num_correct}/ {num_samples} with accuracy {float(num_correct) / float(num_samples) * 100 : .2f}')\n",
    "        \n",
    "check_accuracy(train_loader, model)\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36fb977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2Seq Model\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "0fe077fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "import numpy as np\n",
    "import spacy\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "272cb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_ger = de_core_news_sm .load()\n",
    "spacy_eng = en_core_web_sm .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f31be65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_ger(text):\n",
    "    return [tok.text for tok in spacy_ger.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f02a18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_eng(text):\n",
    "    return [tok.text for tok in spacy_eng.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "12a6016f",
   "metadata": {},
   "outputs": [],
   "source": [
    "german = Field(tokenize = tokenizer_ger, lower = True, init_token = '<sos>' , eos_token = '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f448b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "english = Field(tokenize = tokenizer_eng, lower = True, init_token = '<sos>' , eos_token = '<eos>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "00e2268b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = Multi30k(split=('train', 'valid','test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f8f0bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "german.build_vocab(train_data, max_size = 10000, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b27ba736",
   "metadata": {},
   "outputs": [],
   "source": [
    "english.build_vocab(train_data, max_size = 10000, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e0971a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers  = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape : (seq_length, N)\n",
    "        \n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, N, embedding_size)         \n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout = p)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # shape of x : (N) but we want (1, N)\n",
    "        x = x.unsqueeze(0)\n",
    "        \n",
    "        embedding = nn.Embedding(self.embedding(x))\n",
    "        # embedding shape : (1, N, embedding_size)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # shape of outputs : (1, N, hidden_size)\n",
    "        \n",
    "        predictions = self.fc(outputs)\n",
    "        # shape of predictions : (1, N, length_of_vocab)\n",
    "        \n",
    "        predictions = predictions.squeeze(0)\n",
    "        \n",
    "        return predctions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "        def __init__(self, encoder, decoder):\n",
    "            super(Seq2Seq, self).__init__()\n",
    "            self.encoder = encoder\n",
    "            self.decoder = decoder\n",
    "        \n",
    "        def forward(self, source, target, teacher_force_ratio = 0.5):\n",
    "            batch_size = source.shape[1]\n",
    "            target_len = target.shape[0]\n",
    "            target_vocab_size = len(english.vocab)\n",
    "            \n",
    "            outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "            \n",
    "            hidden, cell = self.encoder(source)\n",
    "            \n",
    "            # Grab start token \n",
    "            x = target[0]\n",
    "            \n",
    "            for t in range(1, target_len):\n",
    "                output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "                \n",
    "                outputs[t] = output\n",
    "                \n",
    "                \n",
    "                # output shape : (N, english_vocab_size)\n",
    "                best_guess = output.argmax(1)\n",
    "                \n",
    "                x = target[t] if random.random() < teacher_force_ratio else best_guess\n",
    "            \n",
    "            return ouputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "4c10ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparamters\n",
    "num_epochs = 20\n",
    "learning_rate = .001\n",
    "batch_size = 64\n",
    "\n",
    "# Model hyperparameters\n",
    "load_model = False\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_size_encoder = len(german.vocab)\n",
    "input_size_decoder = len(english.vocab)\n",
    "output_size = len(english.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 2\n",
    "enc_dropout = 0.5\n",
    "dec_dropout = 0.5\n",
    "\n",
    "# Tensorboard\n",
    "writer = SummaryWriter(f'runs/Loss_plot')\n",
    "step = 0\n",
    "\n",
    "# train_iterator, validation_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), \n",
    "#                                                                             batch_size = batch_size,\n",
    "#                                                                             sort_within_batch = True,\n",
    "#                                                                             sort_key = lambda x: len(x.src),\n",
    "#                                                                             device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b98d7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "    return torch.tensor(label_list), pad_sequence(text_list, padding_value=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b63f2de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(train_iter)\n",
    "def batch_sampler():\n",
    "    indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(train_list)]\n",
    "    random.shuffle(indices)\n",
    "    pooled_indices = []\n",
    "    # create pool of indices with similar lengths \n",
    "    for i in range(0, len(indices), batch_size * 100):\n",
    "        pooled_indices.extend(sorted(indices[i:i + batch_size * 100], key=lambda x: x[1]))\n",
    "\n",
    "    pooled_indices = [x[0] for x in pooled_indices]\n",
    "\n",
    "    # yield indices for current batch\n",
    "    for i in range(0, len(pooled_indices), batch_size):\n",
    "        yield pooled_indices[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e2b7c9e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bucket_dataloader = DataLoader(list(train_iter), batch_sampler=batch_sampler(),\n",
    "                               collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "780ad956",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(input_size_decoder, decoder_embedding_size, hidden_size, output_size, num_layers, dec_dropout).to(device)\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e3037093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0 / 20]\n",
      "Epoch[1 / 20]\n",
      "Epoch[2 / 20]\n",
      "Epoch[3 / 20]\n",
      "Epoch[4 / 20]\n",
      "Epoch[5 / 20]\n",
      "Epoch[6 / 20]\n",
      "Epoch[7 / 20]\n",
      "Epoch[8 / 20]\n",
      "Epoch[9 / 20]\n",
      "Epoch[10 / 20]\n",
      "Epoch[11 / 20]\n",
      "Epoch[12 / 20]\n",
      "Epoch[13 / 20]\n",
      "Epoch[14 / 20]\n",
      "Epoch[15 / 20]\n",
      "Epoch[16 / 20]\n",
      "Epoch[17 / 20]\n",
      "Epoch[18 / 20]\n",
      "Epoch[19 / 20]\n"
     ]
    }
   ],
   "source": [
    "pad_idx = english.vocab.stoi['<pad>']\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = pad_idx)\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch[{epoch} / {num_epochs}]')\n",
    "    checkpoint = {'state_dict' : model.state_dict(), 'optimizer' : optimizer.state_dict()}\n",
    "    \n",
    "    for batch_idx, batch in enumerate(bucket_dataloader):\n",
    "        inp_data = batch.src.to(device)\n",
    "        target = batch.trg.to(device)\n",
    "        \n",
    "        output = model(inp_data, target)\n",
    "        # output shape : (trg_len, batch_size, output_dim)\n",
    "        \n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        target = target[1:].reshape(-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1)\n",
    "        optimizer.step()\n",
    "        \n",
    "        writer.add_scalar('Training Loss', loss, global_step = step)\n",
    "        step+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d73477dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vision Transformer\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ff637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb20d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    ''' Split images into patches and then embed them\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    img_size : int\n",
    "        Size of the image (it is a square)\n",
    "        \n",
    "    patch_size : int\n",
    "        Size of the patch (it is a square)\n",
    "        \n",
    "    in_chans : int\n",
    "        Number of input channels\n",
    "        \n",
    "    embed_dim : int\n",
    "        The embedding dimension\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    \n",
    "    num_patches : int\n",
    "        Number of patches inside of our image\n",
    "    \n",
    "    proj : nn.Conv2d\n",
    "        Convolutional layer that does both the splitting into patches and their embedding\n",
    "    '''\n",
    "    def __init__(self, img_size, patch_size, in_chans = 3, embed_dim = 768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size = patch_size, stride = patch_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, in_chans, img_size, img_size)'\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        torch.Tensor\n",
    "            Shape '(n_samples, n_patches, embed_dim)'\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        x = self.proj(x) \n",
    "        # (n_samples, embed_dim, num_patches ** 0.5, num_patches ** 0.5)         \n",
    "\n",
    "        x = x.flatten(2)\n",
    "        # (n_samples, embed_dim, num_patches)\n",
    "        \n",
    "        x = x.transpose(1, 2)\n",
    "        # (n_samples, num_patches, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204635d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    '''Attention mechanism\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dim : int\n",
    "        The input and output dimension of per token feature.\n",
    "    \n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "    \n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to the query, key and value operations\n",
    "    \n",
    "    attn_p : float\n",
    "        Dropout probability applied to the query, key and value tensors\n",
    "    \n",
    "    proj_p : float\n",
    "        Dropout probability applied to the output tensor\n",
    "        \n",
    "    Attributes \n",
    "    ----------\n",
    "    \n",
    "    scale : float\n",
    "        Normalizing constant for the dot product\n",
    "    \n",
    "    qkv : nn.Linear \n",
    "        Linear projection for the query, key and values\n",
    "    \n",
    "    proj : nn.Linear\n",
    "        Linear mapping that takes in the concatenated output of all attention heads and maps it into a new space\n",
    "        \n",
    "    attn_drop, proj_drop : nn.Dropout\n",
    "        Dropout layers\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim, n_heads = 12, qkv_bias = True, attn_p = 0, proj_p = 0):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.dim = dim\n",
    "        self.head_dim = dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        \n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias = qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, n_patches + 1, dim)'\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape '(n_samples, n_patches + 1, dim)'\n",
    "        '''\n",
    "        \n",
    "        n_samples, n_tokens, dim = x.shape\n",
    "        \n",
    "        if (dim != self.dim):\n",
    "            raise ValueError\n",
    "        \n",
    "        qkv = self.qkv(x)\n",
    "        # (n_samples, n_patches + 1, 3 * dim)\n",
    "        \n",
    "        qkv = qkv.reshape(n_samples, n_tokens, 3, self.n_heads, self.head_dim)\n",
    "        # (n_samples, n_patches + 1, 3, n_heads, head_dim)\n",
    "        \n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4)\n",
    "        # (3, n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        \n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        \n",
    "        k_t = k.transpose(-2, -1) \n",
    "        #(n_samples, n_heads, head_dim, n_patches + 1)\n",
    "        \n",
    "        dp = (q @ k_t) * self.scale\n",
    "        #(n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        \n",
    "        attn = dp.softmax(dim = -1)\n",
    "        #(n_samples, n_heads, n_patches + 1, n_patches + 1)\n",
    "        \n",
    "        weighted_avg = attn @ v\n",
    "        #(n_samples, n_heads, n_patches + 1, head_dim)\n",
    "        \n",
    "        weighted_avg = weighted_avg.transpose(1, 2)\n",
    "        #(n_samples, n_patches + 1, n_heads, head_dim)\n",
    "        \n",
    "        weighted_avg = weighted_avg.flatten(2)\n",
    "        #(n_samples, n_patches + 1, dim)\n",
    "        \n",
    "        x = self.proj(weighted_avg)\n",
    "        #(n_samples, n_patches + 1, dim)\n",
    "        \n",
    "        x = self.proj_drop(x)\n",
    "        #(n_samples, n_patches + 1, dim)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12159e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''Multilayer Perceptron\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    in_features : int\n",
    "        Number of input features\n",
    "    \n",
    "    hidden_features : int\n",
    "        Number of nodes in the hidden layer\n",
    "        \n",
    "    out_features : int\n",
    "        Number of output features\n",
    "        \n",
    "    p : float\n",
    "        Dropout probabilty\n",
    "        \n",
    "        \n",
    "    Attribute\n",
    "    ---------\n",
    "    \n",
    "    fc : nn.Linear\n",
    "         The first linear layer\n",
    "    \n",
    "    act : nn.GELU\n",
    "        GELU activation function\n",
    "        \n",
    "    fc2 : nn.Linear \n",
    "        The second linear layer\n",
    "        \n",
    "    drop : nn.Dropout\n",
    "        Dropout layer\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, in_features, hidden_features, out_features, p=0.):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = nn.GELU()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(p)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches+1, in_features)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches+1, out_features)\n",
    "        '''\n",
    "        \n",
    "        x = self.fc1(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.act(x)  # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.drop(x) # (n_samples, n_patches+1, hidden_features)\n",
    "        x = self.fc2(x)  # (n_samples, n_patches+1, out_features)\n",
    "        x = self.drop(x) # (n_samples, n_patches+1, out_features)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b62a9a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    '''Transformer Block\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    dim : int\n",
    "        Embedding dimension\n",
    "    \n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "        \n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP' module with respect to `dim'\n",
    "        \n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to query, key and value projections.\n",
    "        \n",
    "    p, attn_p : float\n",
    "        Dropout probabilty\n",
    "        \n",
    "        \n",
    "    Attributes\n",
    "    ---------\n",
    "    \n",
    "    norm1, norm2 : LayerNorm\n",
    "         Layer Normalization\n",
    "    \n",
    "    attn : Attention\n",
    "         Attention module\n",
    "        \n",
    "    mlp : MLP \n",
    "         MLP module\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dim, n_heads, mlp_rtio = 4.0, qkv_bias = True, p = 0., attn_p = 0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim, eps = 1e-6)\n",
    "        self.attn  = Attention(dim, n_heads = n_heads, qkv_bias = qkv_bias, attn_p = attn_p, proj_p = p)\n",
    "        self.norm2 = nn.LayerNorm(dim, eps = 1e-6)\n",
    "        hidden_features = int(dim * mlp_ratio)\n",
    "        self.mlp = MLP(in_features = dim, hidden_features = hidden_features, out_features = dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches+1, dim)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, n_patches+1, dim) \n",
    "        '''\n",
    "        \n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c44b6bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    '''Simplified implementation of Vision Transformer\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    img_size : int\n",
    "        Both height and width of an image (it's a square)\n",
    "        \n",
    "    patch_size : int\n",
    "        Both height and width of a patch (it's a square)\n",
    "        \n",
    "    in_chans : int\n",
    "        Number of input channels\n",
    "        \n",
    "    n_classes : int\n",
    "        Number of classes\n",
    "        \n",
    "    embed_dim : int\n",
    "        Dimensionality of the token/patch embeddings\n",
    "        \n",
    "    depth : int\n",
    "        Number of blocks\n",
    "    \n",
    "    n_heads : int\n",
    "        Number of attention heads\n",
    "    \n",
    "    mlp_ratio : float\n",
    "        Determines the hidden dimension of the `MLP' module.\n",
    "    \n",
    "    qkv_bias : bool\n",
    "        If True then we include bias to query, key and value projections.\n",
    "        \n",
    "    p, attn_p : float\n",
    "        Dropout probabilty\n",
    "        \n",
    "        \n",
    "    Attributes\n",
    "    ---------\n",
    "    \n",
    "    patch_embed : PatchEmbed\n",
    "        Instance of `PatchEmbed' Layer\n",
    "        \n",
    "    cls_token   : nn.Parameter\n",
    "         Learnable parameter that will represent the first token in the sequence. It has `embed_dim' elements.\n",
    "        \n",
    "    pos_emb     : nn.Parameter\n",
    "         Positional embedding of the cls token + all the patches.\n",
    "         It has `(n_patches + 1) * embed_dim' elements\n",
    "    \n",
    "    pos_drop    : nn.Dropout\n",
    "        Dropout layer\n",
    "    \n",
    "    blocks      : nn.ModuleList\n",
    "        List of `Block' modules.\n",
    "    \n",
    "    norm        : nn.LayerNorm\n",
    "        Layer Normalization\n",
    "    '''\n",
    "    \n",
    "    def __init__(self,\n",
    "                img_size = 384,\n",
    "                patch_size = 16,\n",
    "                in_chans = 3,\n",
    "                n_classes = 1000,\n",
    "                embed_dim = 786,\n",
    "                depth = 12,\n",
    "                n_heads = 12,\n",
    "                mlp_ratio = 4,\n",
    "                qkv_bias = True,\n",
    "                p = 0.,\n",
    "                attn_p = 0.\n",
    "                ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patch_embed = PatchEmbed(img_size = img_size, patch_size = patch_size, in_chans = in_chans, embed_dim = embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=p)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                Block\n",
    "                (\n",
    "                    dim = embed_dim,\n",
    "                    n_heads = n_heads,\n",
    "                    mlp_ratio = mlp_ratio,\n",
    "                    qkv_bias = qkv_bias,\n",
    "                    p = p,\n",
    "                    attn_p = attn_p                    \n",
    "                )\n",
    "                \n",
    "                for _ in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim, eps = 1e-6)\n",
    "        self.out = nn.Linear(embed_dim, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Run forward pass\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        x : torch.Tensor\n",
    "            Shape `(n_samples, in_chans, img_size, img_size)'\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        logits : torch.Tensor\n",
    "            Logits over all the classes - `(n_samples, n_classes)' \n",
    "        '''\n",
    "        \n",
    "        n_samples = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        cls_token = self.cls_token.expand(n_samples, -1, -1) # (n_samples, 1, embed_dim)\n",
    "        x = torch.cat((cls_token, x), dim = 1) # (n_samples, 1 + n_patches, embed_dim)\n",
    "        \n",
    "        x = x + self.pos_embed # (n_samples, 1 + n_patches, embed_dim)\n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        for block in self.blocks :\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        cls_token_final = x[:, 0]\n",
    "        x = self.out(cls_token_final)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f4a69ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------\n",
    "# ------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "585a5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09875b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "# Then flatten and 4096x4096x1000 linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "078827de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_net(nn.Module):\n",
    "    def __init__(self, in_channels = 3, num_classes = 1000):\n",
    "        super(VGG_net, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layers = self.create_conv_layers(VGG16)\n",
    "        \n",
    "        self.fcs = nn.Sequential(nn.Linear(512 * 7 * 7, 4096),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p=0.5),\n",
    "                                 nn.Linear(4096, 4096),\n",
    "                                 nn.ReLU(),\n",
    "                                 nn.Dropout(p=0.5),\n",
    "                                 nn.Linear(4096, num_classes))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fcs(x)\n",
    "        return x\n",
    "    \n",
    "    def create_conv_layers(self, architecture):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        \n",
    "        for x in architecture:\n",
    "            if(type(x) == int):\n",
    "                out_channels = x\n",
    "                \n",
    "                layers += [nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = (3, 3), stride = (1, 1), padding = (1, 1)), nn.BatchNorm2d(out_channels), nn.ReLU()]\n",
    "                in_channels = x\n",
    "            \n",
    "            elif x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size = (2, 2), stride = (2, 2))]\n",
    "            \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c9d6e481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1000])\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = VGG_net(in_channels = 3, num_classes = 1000).to(device)\n",
    "x = torch.rand(1, 3, 224, 224).to(device)\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "991c7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN-LSTM Image Captioning\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e3822f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bf4b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size, train_CNN = False):\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained = True, aux_logits = False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        features = self.inception(images)\n",
    "        \n",
    "        for name , param in self.inception.named_parameters():\n",
    "            if \"fc.weight\" in name or \"fc.bias\" in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = train_CNN\n",
    "        \n",
    "        return self.dropout(self.relu(features))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33448722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm  = nn.LSTM(embed_size, hidden_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50e4735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNtoRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
    "        super(CNNtoRNN, self).__init__()\n",
    "        self.encoderCNN = EncoderCNN(embed_size)\n",
    "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
    "    \n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        outputs  = self.decoderRNN(features, captions)\n",
    "        return outputs\n",
    "    \n",
    "    def caption_image(self, image, vocabulary, max_length = 50):\n",
    "        result_caption = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0)\n",
    "            states = None\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hiddens.unsqueeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "                \n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "                \n",
    "                if(vocabulary.itos[predicted.item()] == \"<EOS>\"):\n",
    "                    break\n",
    "                    \n",
    "            return [vocabulary.itos[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98eeac3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No Training Loop for the architecure yet. Will soon come back to this\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8465ccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple GAN\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f170ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77423427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, img_dim):\n",
    "        super().__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "                                    nn.Linear(img_dim, 128),\n",
    "                                    nn.LeakyReLU(0.1),\n",
    "                                    nn.Linear(128, 1),\n",
    "                                    nn.Sigmoid()\n",
    "                                 )\n",
    "    def forward(self, x):\n",
    "            return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb0e781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, img_dim):\n",
    "        super().__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "                                    nn.Linear(z_dim, 256),\n",
    "                                    nn.LeakyReLU(0.1),\n",
    "                                    nn.Linear(256, img_dim),\n",
    "                                    nn.Tanh()\n",
    "                                )\n",
    "        \n",
    "    def forward(self, x):\n",
    "            return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff3ee817",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "lr = 3e-4\n",
    "z_dim = 64\n",
    "image_dim = 784\n",
    "batch_size = 32\n",
    "num_epochs = 50\n",
    "\n",
    "disc = Discriminator(image_dim).to(device)\n",
    "gen =  Generator(z_dim, image_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ab95cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn((batch_size, z_dim)).to(device)\n",
    "transforms  = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,) , (0.5,))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc38dbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset  = datasets.MNIST(root=\"dataset/\" , transform = transforms, download = True)\n",
    "loader   = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = lr)\n",
    "opt_gen  = optim.Adam(gen.parameters(), lr = lr)\n",
    "criterion = nn.BCELoss()\n",
    "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/GAN_MNIST/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b835aa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0 / 50] \\ Loss D :  0.6131 , Loss G :  0.7316\n",
      "Epoch[1 / 50] \\ Loss D :  0.6359 , Loss G :  0.8937\n",
      "Epoch[2 / 50] \\ Loss D :  0.3602 , Loss G :  1.3473\n",
      "Epoch[3 / 50] \\ Loss D :  0.9751 , Loss G :  0.6499\n",
      "Epoch[4 / 50] \\ Loss D :  0.5774 , Loss G :  0.9267\n",
      "Epoch[5 / 50] \\ Loss D :  0.6263 , Loss G :  0.9647\n",
      "Epoch[6 / 50] \\ Loss D :  0.6542 , Loss G :  0.9407\n",
      "Epoch[7 / 50] \\ Loss D :  0.4121 , Loss G :  1.5104\n",
      "Epoch[8 / 50] \\ Loss D :  0.5918 , Loss G :  1.4942\n",
      "Epoch[9 / 50] \\ Loss D :  0.8102 , Loss G :  0.7931\n",
      "Epoch[10 / 50] \\ Loss D :  0.5829 , Loss G :  0.8360\n",
      "Epoch[11 / 50] \\ Loss D :  0.5085 , Loss G :  1.2615\n",
      "Epoch[12 / 50] \\ Loss D :  0.7817 , Loss G :  0.9677\n",
      "Epoch[13 / 50] \\ Loss D :  0.7596 , Loss G :  0.9585\n",
      "Epoch[14 / 50] \\ Loss D :  0.4953 , Loss G :  1.0973\n",
      "Epoch[15 / 50] \\ Loss D :  0.8616 , Loss G :  0.7013\n",
      "Epoch[16 / 50] \\ Loss D :  0.5627 , Loss G :  1.1396\n",
      "Epoch[17 / 50] \\ Loss D :  0.8596 , Loss G :  0.8698\n",
      "Epoch[18 / 50] \\ Loss D :  0.4357 , Loss G :  1.1929\n",
      "Epoch[19 / 50] \\ Loss D :  0.9300 , Loss G :  0.7360\n",
      "Epoch[20 / 50] \\ Loss D :  0.6855 , Loss G :  1.0129\n",
      "Epoch[21 / 50] \\ Loss D :  0.7545 , Loss G :  0.8931\n",
      "Epoch[22 / 50] \\ Loss D :  0.7299 , Loss G :  0.9290\n",
      "Epoch[23 / 50] \\ Loss D :  0.7329 , Loss G :  1.0908\n",
      "Epoch[24 / 50] \\ Loss D :  0.5818 , Loss G :  1.1541\n",
      "Epoch[25 / 50] \\ Loss D :  0.5959 , Loss G :  1.2510\n",
      "Epoch[26 / 50] \\ Loss D :  0.6889 , Loss G :  1.3222\n",
      "Epoch[27 / 50] \\ Loss D :  0.5151 , Loss G :  1.5617\n",
      "Epoch[28 / 50] \\ Loss D :  0.8268 , Loss G :  0.8863\n",
      "Epoch[29 / 50] \\ Loss D :  0.7348 , Loss G :  0.8035\n",
      "Epoch[30 / 50] \\ Loss D :  0.7596 , Loss G :  1.1319\n",
      "Epoch[31 / 50] \\ Loss D :  0.6374 , Loss G :  0.8974\n",
      "Epoch[32 / 50] \\ Loss D :  0.5330 , Loss G :  1.1078\n",
      "Epoch[33 / 50] \\ Loss D :  0.5378 , Loss G :  1.2741\n",
      "Epoch[34 / 50] \\ Loss D :  0.8384 , Loss G :  0.7238\n",
      "Epoch[35 / 50] \\ Loss D :  0.5161 , Loss G :  1.1533\n",
      "Epoch[36 / 50] \\ Loss D :  0.6414 , Loss G :  1.0357\n",
      "Epoch[37 / 50] \\ Loss D :  0.6205 , Loss G :  0.9329\n",
      "Epoch[38 / 50] \\ Loss D :  0.5805 , Loss G :  1.3393\n",
      "Epoch[39 / 50] \\ Loss D :  0.6740 , Loss G :  1.1929\n",
      "Epoch[40 / 50] \\ Loss D :  0.7388 , Loss G :  0.9673\n",
      "Epoch[41 / 50] \\ Loss D :  0.6826 , Loss G :  0.9370\n",
      "Epoch[42 / 50] \\ Loss D :  0.5904 , Loss G :  1.0930\n",
      "Epoch[43 / 50] \\ Loss D :  0.7039 , Loss G :  0.9408\n",
      "Epoch[44 / 50] \\ Loss D :  0.7168 , Loss G :  1.0144\n",
      "Epoch[45 / 50] \\ Loss D :  0.6714 , Loss G :  0.7158\n",
      "Epoch[46 / 50] \\ Loss D :  0.5697 , Loss G :  1.1660\n",
      "Epoch[47 / 50] \\ Loss D :  0.6781 , Loss G :  0.9787\n",
      "Epoch[48 / 50] \\ Loss D :  0.6088 , Loss G :  0.8820\n",
      "Epoch[49 / 50] \\ Loss D :  0.6752 , Loss G :  0.9701\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx , (real , _) in enumerate(loader):\n",
    "        real = real.view(-1, 784).to(device)\n",
    "        batch_size = real.shape[0]\n",
    "        \n",
    "        # Train Discriminator : max log(D(real)) +  log(1 - D(G(z)))\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        fake  = gen(noise)\n",
    "        disc_real  = disc(real).view(-1)\n",
    "        disc_loss_real = criterion(disc_real , torch.ones_like(disc_real)) #  log(D(real)) {min of negative of this}\n",
    "        \n",
    "        disc_fake  = disc(fake).view(-1)\n",
    "        disc_loss_fake = criterion(disc_fake, torch.zeros_like(disc_fake)) #  log((1 - D(G(z)))) {min of negative of this}\n",
    "        \n",
    "        loss_disc = (disc_loss_real + disc_loss_fake) / 2\n",
    "        opt_disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph = True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "        # Train Generator : min log(1 - D(G(z)))  <-->  max log(D(G(z)))\n",
    "        output = disc(fake).view(-1)\n",
    "        gen_loss = criterion(output , torch.ones_like(output))\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        gen_loss.backward()\n",
    "        opt_gen.step()\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            print(\n",
    "                    f\"Epoch[{epoch} / {num_epochs}] \\n\"\n",
    "                    f\"Loss D : {loss_disc : .4f} , Loss G : {gen_loss : .4f}\" \n",
    "                 )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise).reshape(-1, 1, 28, 28)\n",
    "                data = real.reshape(-1, 1, 28, 28)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake, normalize = True)\n",
    "                img_grid_real = torchvision.utils.make_grid(data, normalize = True)\n",
    "                \n",
    "                writer_fake.add_image(\"Mnist Fake Images\", img_grid_fake, global_step = step)\n",
    "                writer_real.add_image(\"Mnist Real Images\", img_grid_real, global_step = step)\n",
    "                \n",
    "                step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5c2733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c646b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc231b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        # input shape : N x channels_img x 64 x 64\n",
    "        \n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "                                     nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # 32 x 32 \n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     self._block(features_d, features_d * 2, 4, 2, 1),     # 16 x 16\n",
    "                                     self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8\n",
    "                                     self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4 x 4\n",
    "                                     nn.Conv2d(features_d * 8, 1, kernel_size = 4, stride = 2, padding = 0), # 1 x 1\n",
    "                                     nn.Sigmoid()\n",
    "                                 )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                             nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False), \n",
    "                             nn.BatchNorm2d(out_channels), \n",
    "                             nn.LeakyReLU(0.2)\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cf6dcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        # input shape : N x z_dim x 1 x 1\n",
    "        \n",
    "        '''\n",
    "        ------------------------------\n",
    "        nn.ConvTranspose2d output shape ---> s * (n-1) + f - 2p\n",
    "        '''\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "                                     self._block(z_dim, features_g * 16, 4, 1, 0),     # N x features_g * 16 x 4 x 4\n",
    "                                     self._block(features_g * 16, features_g * 8, 4, 2, 1), # 8 x 8\n",
    "                                     self._block(features_g * 8, features_g * 4, 4, 2, 1),  # 16 x 16\n",
    "                                     self._block(features_g * 4, features_g * 2, 4, 2, 1),  # 32 x 32\n",
    "                                     nn.ConvTranspose2d(features_g * 2, channels_img, kernel_size = 4, stride = 2, padding = 1), # 64 x 64\n",
    "                                     nn.Tanh()\n",
    "                                 )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                             nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias = False), \n",
    "                             nn.BatchNorm2d(out_channels), \n",
    "                             nn.ReLU()\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4e2c4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad2381e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N, in_channels, H, W = 8, 3, 64, 64\n",
    "z_dim = 100\n",
    "x = torch.randn((N, in_channels, H, W))\n",
    "disc = Discriminator(in_channels, 8)\n",
    "initialize_weights(disc)\n",
    "assert disc(x).shape == (N, 1, 1, 1)\n",
    "\n",
    "gen = Generator(z_dim, in_channels, 8)\n",
    "initialize_weights(gen)\n",
    "z = torch.randn((N, z_dim, 1, 1))\n",
    "assert gen(z).shape == (N, in_channels, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e8cd29e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------Training-------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b675f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fadd242",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "learning_rate = 2e-4\n",
    "batch_size = 128\n",
    "image_size = 64\n",
    "channels_img = 1\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b747926",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5 for _ in range(channels_img)], [0.5 for _ in range(channels_img)])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92d99f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.MNIST(root = \"dataset/\", train = True, transform = transforms, download = True)\n",
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "gen = Generator(z_dim, channels_img, features_gen).to(device)\n",
    "disc = Discriminator(channels_img, features_disc).to(device)\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8050345",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = learning_rate, betas = (0.5, 0.999))\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33f3ea58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.rand(32, z_dim, 1, 1).to(device)\n",
    "writer_fake = SummaryWriter(f\"runs/DCGAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/DCGAN_MNIST/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc41b6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 5] Batch 0 / 469 \\ Loss D :  0.5612 , Loss G :  0.9401\n",
      "Epoch [0 / 5] Batch 100 / 469 \\ Loss D :  0.0144 , Loss G :  4.1656\n",
      "Epoch [0 / 5] Batch 200 / 469 \\ Loss D :  0.6813 , Loss G :  0.7287\n",
      "Epoch [0 / 5] Batch 300 / 469 \\ Loss D :  0.6353 , Loss G :  0.2488\n",
      "Epoch [0 / 5] Batch 400 / 469 \\ Loss D :  0.4466 , Loss G :  1.2437\n",
      "Epoch [1 / 5] Batch 0 / 469 \\ Loss D :  0.5299 , Loss G :  1.5852\n",
      "Epoch [1 / 5] Batch 100 / 469 \\ Loss D :  0.6916 , Loss G :  0.6671\n",
      "Epoch [1 / 5] Batch 200 / 469 \\ Loss D :  0.8239 , Loss G :  1.3765\n",
      "Epoch [1 / 5] Batch 300 / 469 \\ Loss D :  0.5727 , Loss G :  0.9257\n",
      "Epoch [1 / 5] Batch 400 / 469 \\ Loss D :  0.5700 , Loss G :  0.9649\n",
      "Epoch [2 / 5] Batch 0 / 469 \\ Loss D :  0.5810 , Loss G :  1.1754\n",
      "Epoch [2 / 5] Batch 100 / 469 \\ Loss D :  0.7114 , Loss G :  0.8233\n",
      "Epoch [2 / 5] Batch 200 / 469 \\ Loss D :  0.6026 , Loss G :  1.5578\n",
      "Epoch [2 / 5] Batch 300 / 469 \\ Loss D :  0.5099 , Loss G :  2.0573\n",
      "Epoch [2 / 5] Batch 400 / 469 \\ Loss D :  0.4635 , Loss G :  1.2295\n",
      "Epoch [3 / 5] Batch 0 / 469 \\ Loss D :  0.5109 , Loss G :  1.3212\n",
      "Epoch [3 / 5] Batch 100 / 469 \\ Loss D :  0.4200 , Loss G :  2.4925\n",
      "Epoch [3 / 5] Batch 200 / 469 \\ Loss D :  0.3762 , Loss G :  1.0169\n",
      "Epoch [3 / 5] Batch 300 / 469 \\ Loss D :  0.3881 , Loss G :  1.6619\n",
      "Epoch [3 / 5] Batch 400 / 469 \\ Loss D :  0.2187 , Loss G :  1.6251\n",
      "Epoch [4 / 5] Batch 0 / 469 \\ Loss D :  0.2170 , Loss G :  1.8382\n",
      "Epoch [4 / 5] Batch 100 / 469 \\ Loss D :  0.3404 , Loss G :  3.4686\n",
      "Epoch [4 / 5] Batch 200 / 469 \\ Loss D :  0.1037 , Loss G :  2.6373\n",
      "Epoch [4 / 5] Batch 300 / 469 \\ Loss D :  0.5055 , Loss G :  2.1530\n",
      "Epoch [4 / 5] Batch 400 / 469 \\ Loss D :  0.0881 , Loss G :  4.4154\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "        fake = gen(noise)\n",
    "        \n",
    "#       Train Discriminator max log(D(x)) + log(1 - D(G(z)))  \n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        disc_real_loss = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        disc_fake_loss = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        \n",
    "        opt_disc.zero_grad()\n",
    "        disc_loss.backward(retain_graph = True)\n",
    "        opt_disc.step()\n",
    "        \n",
    "#       Train Generator min log(1 - D(G(z))) <---> max log(D(G(z))) \n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        gen_fake_loss = criterion(disc_fake, torch.ones_like(disc_fake))\n",
    "        \n",
    "        opt_gen.zero_grad()\n",
    "        gen_fake_loss.backward()\n",
    "        opt_gen. step()\n",
    "        \n",
    "        if(batch_idx % 100 == 0):\n",
    "            print(f\"Epoch [{epoch} / {num_epochs}] Batch {batch_idx} / {len(loader)} \\ Loss D : {disc_loss : .4f} , Loss G : {gen_fake_loss : .4f}\")\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                \n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32] , normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32] , normalize = True)\n",
    "                \n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step = step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step = step)\n",
    "                \n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50508924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------WGAN------------------------------------------------------\n",
    "# Using the same model as DCGAN, just a different training loop\n",
    "# -------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b11428b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN without gradient penalty\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        # input shape : N x channels_img x 64 x 64\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "                                     nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # 32 x 32 \n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     self._block(features_d, features_d * 2, 4, 2, 1),     # 16 x 16\n",
    "                                     self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8\n",
    "                                     self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4 x 4\n",
    "                                     nn.Conv2d(features_d * 8, 1, kernel_size = 4, stride = 2, padding = 0) # 1 x 1\n",
    "                                 )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                             nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False), \n",
    "                             nn.BatchNorm2d(out_channels), \n",
    "                             nn.LeakyReLU(0.2)\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a99eb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Critic(channels_img, features_disc).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7176802b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 5e-5\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "channels_img = 1\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "critic_iterations = 5\n",
    "weight_clip = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f5149f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.RMSprop(gen.parameters(), lr = learning_rate)\n",
    "opt_disc = optim.RMSprop(disc.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "633edf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.rand(32, z_dim, 1, 1).to(device)\n",
    "writer_fake = SummaryWriter(f\"runs/WGAN_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/WGAN_MNIST/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bd06bce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 5] Batch 0 / 469 \\ Loss D : -0.3524 , Loss G :  0.4251\n",
      "Epoch [0 / 5] Batch 100 / 469 \\ Loss D : -0.3282 , Loss G :  0.0815\n",
      "Epoch [0 / 5] Batch 200 / 469 \\ Loss D : -0.3025 , Loss G :  0.3802\n",
      "Epoch [0 / 5] Batch 300 / 469 \\ Loss D : -0.3584 , Loss G :  0.3346\n",
      "Epoch [0 / 5] Batch 400 / 469 \\ Loss D : -0.3593 , Loss G :  0.4416\n",
      "Epoch [1 / 5] Batch 0 / 469 \\ Loss D : -0.3954 , Loss G :  0.1303\n",
      "Epoch [1 / 5] Batch 100 / 469 \\ Loss D : -0.3537 , Loss G :  0.2726\n",
      "Epoch [1 / 5] Batch 200 / 469 \\ Loss D : -0.3252 , Loss G :  0.3859\n",
      "Epoch [1 / 5] Batch 300 / 469 \\ Loss D : -0.4189 , Loss G :  0.0387\n",
      "Epoch [1 / 5] Batch 400 / 469 \\ Loss D : -0.4580 , Loss G :  0.2559\n",
      "Epoch [2 / 5] Batch 0 / 469 \\ Loss D : -0.4156 , Loss G :  0.0529\n",
      "Epoch [2 / 5] Batch 100 / 469 \\ Loss D : -0.4690 , Loss G :  0.0851\n",
      "Epoch [2 / 5] Batch 200 / 469 \\ Loss D : -0.4553 , Loss G :  0.1036\n",
      "Epoch [2 / 5] Batch 300 / 469 \\ Loss D : -0.4039 , Loss G :  0.4420\n",
      "Epoch [2 / 5] Batch 400 / 469 \\ Loss D : -0.5519 , Loss G :  0.3357\n",
      "Epoch [3 / 5] Batch 0 / 469 \\ Loss D : -0.4142 , Loss G :  0.4399\n",
      "Epoch [3 / 5] Batch 100 / 469 \\ Loss D : -0.4507 , Loss G :  0.4583\n",
      "Epoch [3 / 5] Batch 200 / 469 \\ Loss D : -0.4523 , Loss G :  0.4271\n",
      "Epoch [3 / 5] Batch 300 / 469 \\ Loss D : -0.4582 , Loss G : -0.0160\n",
      "Epoch [3 / 5] Batch 400 / 469 \\ Loss D : -0.4542 , Loss G :  0.4262\n",
      "Epoch [4 / 5] Batch 0 / 469 \\ Loss D : -0.5734 , Loss G :  0.2984\n",
      "Epoch [4 / 5] Batch 100 / 469 \\ Loss D : -0.3601 , Loss G :  0.3980\n",
      "Epoch [4 / 5] Batch 200 / 469 \\ Loss D : -0.5081 , Loss G :  0.1187\n",
      "Epoch [4 / 5] Batch 300 / 469 \\ Loss D : -0.4566 , Loss G :  0.1694\n",
      "Epoch [4 / 5] Batch 400 / 469 \\ Loss D : -0.4833 , Loss G :  0.2880\n"
     ]
    }
   ],
   "source": [
    "# Training WGAN without gradient penalty\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        \n",
    "        for _ in range(critic_iterations):\n",
    "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "            fake = gen(noise)\n",
    "        \n",
    "    #       Train Critic max E[critic(x)] - E[critic(gen_fake)]  \n",
    "            disc_real = disc(real).reshape(-1)\n",
    "            disc_fake = disc(fake).reshape(-1)\n",
    "            disc_loss = -(torch.mean(disc_real) - torch.mean(disc_fake))\n",
    "\n",
    "            opt_disc.zero_grad()\n",
    "            disc_loss.backward(retain_graph = True)\n",
    "            opt_disc.step()\n",
    "            \n",
    "            for p in disc.parameters():\n",
    "                p.data.clamp_(-weight_clip, weight_clip)\n",
    "\n",
    "#       Train Generator min -E[critic(gen_fake)] \n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        gen_fake_loss = -torch.mean(disc_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        gen_fake_loss.backward()\n",
    "        opt_gen. step()\n",
    "\n",
    "        if(batch_idx % 100 == 0):\n",
    "            print(f\"Epoch [{epoch} / {num_epochs}] Batch {batch_idx} / {len(loader)} \\ Loss D : {disc_loss : .4f} , Loss G : {gen_fake_loss : .4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32] , normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32] , normalize = True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step = step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step = step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "969312a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WGAN with gradient penalty\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, channels_img, features_d):\n",
    "        # input shape : N x channels_img x 64 x 64\n",
    "        \n",
    "        super(Critic, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "                                     nn.Conv2d(channels_img, features_d, kernel_size = 4, stride = 2, padding = 1), # 32 x 32 \n",
    "                                     nn.LeakyReLU(0.2),\n",
    "                                     self._block(features_d, features_d * 2, 4, 2, 1),     # 16 x 16\n",
    "                                     self._block(features_d * 2, features_d * 4, 4, 2, 1), # 8 x 8\n",
    "                                     self._block(features_d * 4, features_d * 8, 4, 2, 1),  # 4 x 4\n",
    "                                     nn.Conv2d(features_d * 8, 1, kernel_size = 4, stride = 2, padding = 0) # 1 x 1\n",
    "                                 )\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "                             nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias = False), \n",
    "                             nn.InstanceNorm2d(out_channels, affine = True), \n",
    "                             nn.LeakyReLU(0.2)\n",
    "                            )\n",
    "    def forward(self, x):\n",
    "        return self.disc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4e3eccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "gen = Generator(z_dim, channels_img, features_gen).to(device)\n",
    "disc = Discriminator(channels_img, features_disc).to(device)\n",
    "\n",
    "initialize_weights(gen)\n",
    "initialize_weights(disc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8841973a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient penalty\n",
    "\n",
    "def gradient_penalty(disc, real, fake, device = \"cpu\"):\n",
    "    \n",
    "    batch_size, c, h, w = real.shape\n",
    "    epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, c, h, w).to(device)\n",
    "    interpolated_images = real * epsilon + fake * (1 - epsilon)\n",
    "    \n",
    "#   calculate critic scores\n",
    "    mixed_scores = disc(interpolated_images)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "               inputs = interpolated_images,\n",
    "               outputs = mixed_scores,\n",
    "               grad_outputs = torch.ones_like(mixed_scores),\n",
    "               create_graph = True,\n",
    "               retain_graph = True\n",
    "               )[0]\n",
    "    \n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim = 1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "863669cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "learning_rate = 1e-4\n",
    "batch_size = 64\n",
    "image_size = 64\n",
    "channels_img = 1\n",
    "z_dim = 100\n",
    "num_epochs = 5\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "critic_iterations = 5\n",
    "lambda_gp = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5ee21241",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_gen = optim.Adam(gen.parameters(), lr = learning_rate, betas = (0.0, 0.9))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr = learning_rate, betas = (0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77252e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.rand(64, z_dim, 1, 1).to(device)\n",
    "writer_fake = SummaryWriter(f\"runs/WGANGP_MNIST/fake\")\n",
    "writer_real = SummaryWriter(f\"runs/WGANGP_MNIST/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b378f2a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0 / 5] Batch 0 / 938 \\ Loss D : -0.1797 , Loss G : -0.3483\n",
      "Epoch [0 / 5] Batch 100 / 938 \\ Loss D : -0.3282 , Loss G : -0.5686\n",
      "Epoch [0 / 5] Batch 200 / 938 \\ Loss D : -0.2980 , Loss G : -0.4959\n",
      "Epoch [0 / 5] Batch 300 / 938 \\ Loss D :  0.6585 , Loss G : -0.5405\n",
      "Epoch [0 / 5] Batch 400 / 938 \\ Loss D : -0.0356 , Loss G : -0.6155\n",
      "Epoch [0 / 5] Batch 500 / 938 \\ Loss D :  4.0277 , Loss G : -0.3212\n",
      "Epoch [0 / 5] Batch 600 / 938 \\ Loss D : -0.5443 , Loss G : -0.4086\n",
      "Epoch [0 / 5] Batch 700 / 938 \\ Loss D : -0.1648 , Loss G : -0.4809\n",
      "Epoch [0 / 5] Batch 800 / 938 \\ Loss D : -0.3586 , Loss G : -0.4794\n",
      "Epoch [0 / 5] Batch 900 / 938 \\ Loss D : -0.4290 , Loss G : -0.4260\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [72]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m disc_real \u001b[38;5;241m=\u001b[39m disc(real)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m disc_fake \u001b[38;5;241m=\u001b[39m disc(fake)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m gp \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_penalty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfake\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m disc_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m(torch\u001b[38;5;241m.\u001b[39mmean(disc_real) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(disc_fake)) \u001b[38;5;241m+\u001b[39m lambda_gp \u001b[38;5;241m*\u001b[39m gp\n\u001b[1;32m     17\u001b[0m opt_disc\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Input \u001b[0;32mIn [65]\u001b[0m, in \u001b[0;36mgradient_penalty\u001b[0;34m(disc, real, fake, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     batch_size, c, h, w \u001b[38;5;241m=\u001b[39m real\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m      6\u001b[0m     epsilon \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((batch_size, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, c, h, w)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 7\u001b[0m     interpolated_images \u001b[38;5;241m=\u001b[39m real \u001b[38;5;241m*\u001b[39m epsilon \u001b[38;5;241m+\u001b[39m \u001b[43mfake\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#   calculate critic scores\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     mixed_scores \u001b[38;5;241m=\u001b[39m disc(interpolated_images)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (64) must match the size of tensor b (32) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "# Training WGAN with gradient penalty\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (real, _) in enumerate(loader):\n",
    "        real = real.to(device)\n",
    "        \n",
    "        for _ in range(critic_iterations):\n",
    "            noise = torch.randn((batch_size, z_dim, 1, 1)).to(device)\n",
    "            fake = gen(noise)\n",
    "        \n",
    "    #       Train Critic max E[critic(x)] - E[critic(gen_fake)]  \n",
    "            disc_real = disc(real).reshape(-1)\n",
    "            disc_fake = disc(fake).reshape(-1)\n",
    "            gp = gradient_penalty(disc, real, fake, device = device)\n",
    "            disc_loss = -(torch.mean(disc_real) - torch.mean(disc_fake)) + lambda_gp * gp\n",
    "\n",
    "            opt_disc.zero_grad()\n",
    "            disc_loss.backward(retain_graph = True)\n",
    "            opt_disc.step()\n",
    "            \n",
    "\n",
    "#       Train Generator min -E[critic(gen_fake)] \n",
    "        disc_fake = disc(fake).reshape(-1)\n",
    "        gen_fake_loss = -torch.mean(disc_fake)\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        gen_fake_loss.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if(batch_idx % 100 == 0):\n",
    "            print(f\"Epoch [{epoch} / {num_epochs}] Batch {batch_idx} / {len(loader)} \\ Loss D : {disc_loss : .4f} , Loss G : {gen_fake_loss : .4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32] , normalize = True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32] , normalize = True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step = step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step = step)\n",
    "\n",
    "            step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5f9dd23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- Neural Style Transfer ---------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0b8ca747",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b85fe985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (24): ReLU(inplace=True)\n",
       "  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (26): ReLU(inplace=True)\n",
       "  (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (31): ReLU(inplace=True)\n",
       "  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (33): ReLU(inplace=True)\n",
       "  (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (35): ReLU(inplace=True)\n",
       "  (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.vgg19(pretrained = True).features\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f033b6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG, self).__init__()\n",
    "\n",
    "        self.chosen_features = ['0', '5', '10', '19', '28']\n",
    "        self.model = models.vgg19(pretrained = True).features[:29]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = []\n",
    "        \n",
    "        for layer_num, layer in enumerate(self.model):\n",
    "            x = layer(x)\n",
    "            \n",
    "            if str(layer_num) in self.chosen_features:\n",
    "                features.append(x)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d019fafa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = VGG()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bb872354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_name, loader, device):\n",
    "    image = Image.open(image_name)\n",
    "#     image.convert('L')\n",
    "    print(image.getbands())\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ee95d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "image_size = 356\n",
    "loader = transforms.Compose([transforms.Resize((image_size, image_size)), transforms.ToTensor()])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c223e319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('R', 'G', 'B')\n",
      "('R', 'G', 'B')\n"
     ]
    }
   ],
   "source": [
    "original_image = load_image(\"goth-batman.jpg\", loader, device)\n",
    "style_image = load_image(\"picasso2.jpg\", loader, device)\n",
    "generated_image = original_image.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "57b7e11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 356, 356])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ef904328",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_steps = 6000\n",
    "learning_rate = 0.001\n",
    "alpha = 1\n",
    "beta = 0.01\n",
    "optimizer = optim.Adam([generated_image], lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "324930d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2482648.7500, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(491829.0312, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(165773.3750, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(74086.3281, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(37368.5820, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(20537.1309, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(12738.3027, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(9002.5391, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(7043.1821, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(5879.6885, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(5104.6313, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(4540.7573, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(4107.9355, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3760.5364, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3472.5889, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3228.2664, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(3017.2554, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2833.4734, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2670.3633, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2523.9226, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2391.0222, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2270.9641, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2161.1377, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(2061.1096, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1969.3087, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1888.4705, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1811.9285, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1743.7517, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1678.5552, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "tensor(1620.7953, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for step in range(total_steps):\n",
    "    generated_features = model(generated_image)\n",
    "    original_features  = model(original_image)\n",
    "    style_features     = model(style_image)\n",
    "    \n",
    "    style_loss = original_loss = 0\n",
    "    \n",
    "    for g_feat, o_feat, s_feat in zip(generated_features, original_features, style_features):\n",
    "        batch_size, channels_img, height, width = g_feat.shape\n",
    "        \n",
    "        original_loss += torch.mean((g_feat - o_feat) ** 2)\n",
    "        \n",
    "#       Compute Gram Matrix\n",
    "        \n",
    "        G = g_feat.view(channels_img, height * width).mm(g_feat.view(channels_img, height * width).t())\n",
    "        S = s_feat.view(channels_img, height * width).mm(s_feat.view(channels_img, height * width).t())\n",
    "        \n",
    "        style_loss += torch.mean((G - S) ** 2)\n",
    "        \n",
    "    total_loss = (alpha * original_loss) + (beta * style_loss)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if(step % 200 == 0):\n",
    "        print(total_loss)\n",
    "        save_image(generated_image, \"generated_2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9f9f3aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- LeNet ---------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "a0d8e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "329ed34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.AvgPool2d(kernel_size = (2,2) , stride = (2,2))\n",
    "        self.conv1 = nn.Conv2d(in_channels = 1, out_channels = 6, kernel_size = (5,5), stride = (1,1), padding = (0,0))\n",
    "        self.conv2 = nn.Conv2d(in_channels = 6, out_channels = 16, kernel_size = (5,5), stride = (1,1), padding = (0,0))\n",
    "        self.conv3 = nn.Conv2d(in_channels = 16, out_channels = 120, kernel_size = (5,5), stride = (1,1), padding = (0,0))\n",
    "        self.linear1 = nn.Linear(120 , 84)\n",
    "        self.linear2 = nn.Linear(84 , 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "9cd309cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn((64, 1, 32, 32))\n",
    "model = LeNet()\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d31fefe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------- GoogleNet ---------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "f90816b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5dba8055",
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(conv_block, self).__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, **kwargs)\n",
    "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.batchnorm(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "3f6757b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool):\n",
    "        super(Inception_Block, self).__init__()\n",
    "        \n",
    "        self.branch1 = conv_block(in_channels, out_1x1, kernel_size = 1)\n",
    "        \n",
    "        self.branch2 = nn.Sequential(\n",
    "                                     conv_block(in_channels, red_3x3, kernel_size = 1),\n",
    "                                     conv_block(red_3x3, out_3x3, kernel_size = 3, stride = 1, padding = 1)\n",
    "                                    )\n",
    "        \n",
    "        self.branch3 = nn.Sequential(\n",
    "                                     conv_block(in_channels, red_5x5, kernel_size = 1),\n",
    "                                     conv_block(red_5x5, out_5x5, kernel_size = 5, stride = 1, padding = 2)\n",
    "                                    )\n",
    "        \n",
    "        self.branch4 = nn.Sequential(\n",
    "                                     nn.MaxPool2d(kernel_size = 3, stride = 1, padding = 1),\n",
    "                                     conv_block(in_channels, out_1x1pool, kernel_size = 1)\n",
    "                                    )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return torch.cat([self.branch1(x), self.branch2(x), self.branch3(x), self.branch4(x)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "eb05dc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(nn.Module):\n",
    "    def __init__(self, in_channels = 3, num_channels = 1000):\n",
    "        super(GoogleNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels = in_channels, out_channels = 64, kernel_size = (7,7), stride = (2,2), padding = (3,3))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.conv2 = conv_block(64, 192, kernel_size = 3, stride = 1, padding = 1)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "\n",
    "        #  in_channels, out_1x1, red_3x3, out_3x3, red_5x5, out_5x5, out_1x1pool\n",
    "        self.inception3a = Inception_Block(192, 64, 96, 128, 16, 32, 32)\n",
    "        self.inception3b = Inception_Block(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.inception4a = Inception_Block(480, 192, 96, 208, 16, 48, 64)\n",
    "        self.inception4b = Inception_Block(512, 160, 112, 224, 24, 64, 64)\n",
    "        self.inception4c = Inception_Block(512, 128, 128, 256, 24, 64, 64)\n",
    "        self.inception4d = Inception_Block(512, 112, 144, 288, 32, 64, 64)\n",
    "        self.inception4e = Inception_Block(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.inception5a = Inception_Block(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.inception5b = Inception_Block(832, 384, 192, 384, 48, 128, 128)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(kernel_size = 7, stride = 1)\n",
    "        self.dropout = nn.Dropout(p = 0.4)\n",
    "        self.fc1 = nn.Linear(1024, num_channels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)  \n",
    "        \n",
    "        x = self.inception3a(x)\n",
    "        x = self.inception3b(x)\n",
    "        x = self.maxpool3(x)\n",
    "        \n",
    "        x = self.inception4a(x)\n",
    "        x = self.inception4b(x)\n",
    "        x = self.inception4c(x)\n",
    "        x = self.inception4d(x)\n",
    "        x = self.inception4e(x)\n",
    "        x = self.maxpool4(x)\n",
    "        \n",
    "        x = self.inception5a(x)\n",
    "        x = self.inception5b(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "2469af55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, 224, 224)\n",
    "model = GoogleNet()\n",
    "print(model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588c02aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
